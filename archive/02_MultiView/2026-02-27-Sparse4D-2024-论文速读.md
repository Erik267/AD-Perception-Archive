---
date: 2026-02-27
keywords: [Sparse4D, Sparse Perception, Instance Propagation, 2024]
tags: [Level-02, Perception-Hardcore, Horizon-Robotics]
---

# Sparse4D-è®ºæ–‡é€Ÿè¯»

## 0. åŸºæœ¬ä¿¡æ¯
- **å‘è¡¨æ—¶é—´**: 2024å¹´
- **ä½œè€…å•ä½**: åœ°å¹³çº¿ (Horizon Robotics)
- **ä»£ç ä»“åº“**: [https://github.com/HorizonRobotics/Sparse4D](https://github.com/HorizonRobotics/Sparse4D)
- **Tags**: #ç¨€ç–4Dæ„ŸçŸ¥ #å®ä¾‹ä¼ æ’­ #æä½åŠŸè€— #é‡äº§æ ‡æ†

---

## 1. ğŸ”ª ä»Šæ—¥é”è¯„
> **Sparse4D** æ˜¯æ„ŸçŸ¥ç®—æ³•ä¸­çš„â€œç²¾å‡†å¤–ç§‘æ‰‹æœ¯â€ã€‚
> 
> **æ ¸å¿ƒæ´å¯Ÿ**ï¼šåœ¨ 3D ç©ºé—´ä¸­ï¼Œæœ‰æ„ä¹‰çš„åŒºåŸŸæå…¶ç¨€ç–ï¼ˆä¸åˆ° 1%ï¼‰ã€‚ä¼ ç»Ÿçš„ BEV ç‰¹å¾å›¾æµªè´¹äº† 99% çš„ç®—åŠ›å»è®¡ç®—ç©ºæ—·çš„åœ°é¢ã€‚Sparse4D å½»åº•æ‘’å¼ƒäº† BEV Mapï¼Œç›´æ¥ç»´æŠ¤ä¸€ç»„ **Sparse Instances (ç¨€ç–å®ä¾‹)**ã€‚å®ƒé€šè¿‡ **Instance Propagation** å®ç°äº†çœŸæ­£çš„ 4Dï¼ˆæ—¶ç©ºï¼‰ä¸€è‡´æ€§ã€‚å®ƒæ˜¯å¯¹ç‰¹æ–¯æ‹‰â€œç¨€ç–å æ®â€æ€è·¯çš„ä¸€æ¬¡ç¡¬æ ¸å·¥ç¨‹å›åº”ã€‚

---

## 2. ğŸ—ï¸ æ¨¡å‹æ¶æ„ (Architecture Map)
```mermaid
graph TD
    A[Multi-view Images] --> B[Backbone + FPN]
    B --> C[Sparse Feature Sampling]
    D[Instance Memory] --> E[Instance Propagation]
    E --> F[Transformer Decoder: Decoupled Attn]
    C & F --> G[Instance Refinement]
    G --> H[Final 3D Boxes & Trajectories]
```

### **æ¶æ„ç»†èŠ‚ï¼š**
1. **Input**: 1/8 åˆ° 1/32 çš„å¤šå°ºåº¦ 2D ç‰¹å¾å›¾ã€‚
2. **Instance Modeling**: æ¯ä¸ª Instance åŒ…å«ä¸€ç»„ **3D Anchor Points** å’Œå¯¹åº”çš„ Content Featuresã€‚
3. **Decoupled Attention (æ ¸å¿ƒ)**ï¼šå°†ä½ç½®ä¿¡æ¯ä¸å†…å®¹ç‰¹å¾è§£è€¦å¤„ç†ï¼Œé˜²æ­¢ä½ç½®å™ªå£°æ±¡æŸ“è¯­ä¹‰ç‰¹å¾ã€‚
4. **Iterative Refinement**: é€šè¿‡ 6 å±‚ Decoder ä¸æ–­è°ƒæ•´ Anchor çš„ä½ç½®ï¼Œä½¿å…¶ç´§è´´ç‰©ä½“çœŸå®è¾¹ç¼˜ã€‚

---

## 3. ğŸ’¡ æ ¸å¿ƒåˆ›æ–° (Math & Pseudo-code)

### 3.1 ç¨€ç–é‡‡æ · (Sparse Sampling)
**é€»è¾‘**ï¼šä¸å†è¿›è¡Œå…¨å±€æ³¨æ„ï¼Œä»…åœ¨ 3D Anchor å‘¨å›´è¿›è¡Œé‡‡æ ·ã€‚

**PyTorch é£æ ¼ä¼ªä»£ç å®ç°**ï¼š
```python
def sparse_sampling(anchors, image_feats, cam_params):
    # anchors: [N, 3] (3D åæ ‡)
    # 1. æŠ•å½± 3D ç‚¹åˆ° 2D å¤šè§†è§’å¹³é¢
    # uv_coords: [N, num_views, 2]
    uv_coords = project_3d_to_2d(anchors, cam_params)
    
    # 2. å±€éƒ¨æ’å€¼é‡‡æ · (ä¸å†è®¡ç®—æ•´ä¸ªç‰¹å¾å›¾çš„ Attention)
    # sampled_feats: [N, num_views, C]
    sampled_feats = F.grid_sample(image_feats, uv_coords)
    
    # 3. æƒé‡èšåˆï¼šé€šè¿‡å¯å­¦ä¹ æƒé‡èåˆå¤šè§†è§’ç‰¹å¾
    fused_feat = (sampled_feats * attention_weights).sum(dim=1)
    return fused_feat
```

---

## 5. ğŸ“Š å…³é”®æŒ‡æ ‡ (nuScenes)
- **ç²¾åº¦**: è¾¾åˆ°ä¸ BEVFormer åŒç­‰ç”šè‡³æ›´é«˜çš„ NDS æŒ‡æ ‡ã€‚
- **æ•ˆç‡**: åœ¨ **Orin-X** ä¸Šï¼Œç›¸æ¯” Dense BEV æ–¹æ¡ˆï¼Œæ¨ç†å»¶è¿Ÿé™ä½äº† **40%**ï¼Œæ˜¾å­˜å¸¦å®½å ç”¨é™ä½äº† **70%**ã€‚

---

## 10. ğŸ› ï¸ è½åœ°å»ºè®®
- **ç®—å­é€‚é…**: å…¶æ ¸å¿ƒçš„ `SparseSampling` ç®—å­éå¸¸é€‚åˆä½¿ç”¨åœ°å¹³çº¿ **BPU** æˆ– NVIDIA **DLA** åŠ é€Ÿã€‚
- **é•¿å°¾å¤„ç†**: åœ¨å®ä¾‹åˆå§‹åŒ–æ—¶ï¼Œå»ºè®®ç»“åˆ K-means èšç±»çš„å…ˆéªŒä½ç½®ï¼Œæé«˜åœ¨å†·å¯åŠ¨é˜¶æ®µçš„æ•è·ç‡ã€‚
