---
date: 2026-02-27
keywords: [LSS, Lift-Splat-Shoot, BEV, ECCV 2020, View Transformation]
tags: [Level-02, Perception-Hardcore, BEV-Foundation]
---

# LSS-论文速读

## 0. 基本信息
- **发表时间**: 2020年 (ECCV 2020)
- **作者单位**: NVIDIA (英伟达)
- **代码仓库**: [https://github.com/nv-tlabs/lift-splat-shoot](https://github.com/nv-tlabs/lift-splat-shoot)
- **Tags**: #BEV鼻祖 #多目融合 #隐式深度预测 #端到端先驱

---

## 1. 🔪 今日锐评
> **LSS** 是自动驾驶感知的“分水岭”。
> 
> **核心洞察**：在 LSS 之前，多目融合通常靠后处理拼凑；LSS 之后，全行业转向了**“统一 BEV 空间”**。它最天才的地方在于 **“Lift”** 操作：不追求预测一个精准的深度点，而是预测深度的**离散概率分布**。这种“模糊处理”反而让模型具备了极强的容错性，解决了单目深度恢复的病态问题。

---

## 2. 🏗️ 模型架构 (Architecture Map)
![LSS Pipeline](https://github.com/nv-tlabs/lift-splat-shoot/raw/master/assets/framework.png)
*Figure 1: LSS 官方流程图。展示了从 2D 图像特征到 3D 视锥，再到 BEV 空间的“投影-聚合”过程。*

### **详细文字描述：**
1. **Backbone (2D)**: 对 $N$ 路图像并行提取特征，生成 $F_{2D} \in \mathbb{R}^{N 	imes C 	imes H 	imes W}$。
2. **Lift Module (核心升级)**: 
   - 为每个像素预测一个 $D$ 维的离散深度概率分布。
   - 将 2D 特征沿射线方向“拉伸”，形成 3D 特征视锥（Frustum）。
3. **Splat Module (空间池化)**:
   - 利用相机参数将所有视锥点重投影到统一的 3D 坐标系。
   - 使用 **Voxel Pooling**（体素池化）：将落在同一个 BEV 网格内的特征进行加权求和。
4. **Shoot Module (规划头)**: 基于生成的 BEV Cost-map 评估预设轨迹模板的得分。

---

## 3. 💡 核心创新 (Math & Pseudo-code)

### 3.1 隐式深度投影 (Lift)
**逻辑**：张量外积。每个 2D 像素特征向量与该像素的深度概率分布做外积，生成视锥特征。

**PyTorch 风格伪代码**：
```python
def lift(image_features):
    """
    image_features: [B*N, C, H, W]
    """
    # 1. 预测深度概率 D [B*N, D_bins, H, W]
    # 使用 Softmax 确保深度分布在 0-1 之间
    depth_probs = self.depth_net(image_features).softmax(dim=1) 
    
    # 2. 提取上下文特征 C [B*N, C_feat, H, W]
    context_feats = self.context_net(image_features)
    
    # 3. 核心算子：外积拉伸 (Lift)
    # 通过维度扩展与乘法，将 2D 像素广播到 3D 视锥
    # [B*N, C_feat, D_bins, H, W]
    frustum_features = depth_probs.unsqueeze(1) * context_feats.unsqueeze(2)
    
    return frustum_features
```

### 3.2 Voxel Pooling (Splat)
**逻辑**：将 3D 视锥点云压缩到 BEV 平面。为了加速，作者在代码中使用了 `QuickCumsum` 操作。

---

## 4. 📉 Loss 函数详解
$$L = L_{bev\_seg} + \lambda L_{planning}$$
- **$L_{bev\_seg}$**: 二分类交叉熵 (BCE)，监督车辆、车道线等在 BEV 下的占据掩码。
- **$L_{planning}$ (Shoot)**: **不使用回归**，而是将规划建模为对 $K$ 条预设轨迹的分类问题。使用交叉熵损失监督最优轨迹。
- **注意**: 原生 LSS 不需要 LiDAR 的显式深度监督（Self-supervised Depth）。

---

## 5. 📊 关键指标 (nuScenes)
在 nuScenes 语义分割任务上：
| 类别 | IoU ↑ |
| :--- | :--- |
| Drivable Area | 72.1 |
| Pedestrian Crossing | 32.5 |
| **Mean IoU** | **38.4** |
*注：这是首个证明纯视觉多目 BEV 方案可以达到高可用精度的模型。*

---

## 6. 📂 数据策略与预处理
- **Camera Extrinsics**: 极其依赖准确的外参。在预处理阶段，需要构建图像坐标到 3D 空间的映射 Lookup Table。
- **Augmentation**: 在 BEV 空间进行旋转和缩放时，必须同步调整视锥投影参数。

---

## 7. 🧩 时序与稳定性
- **单帧局限**: 原版 LSS 是单帧的，导致深度概率分布在时间轴上存在小幅跳动。
- **演进**: 后续的 BEVDet 系列通过引入时序对齐显著提升了稳定性。

---

## 8. ⚠️ 长尾与局限
- **算力开销**: `Voxel Pooling` 在早期实现中效率较低（访存不连续），导致推理延迟主要集中在视角转换阶段。
- **远端模糊**: 深度 Bin 随距离增加而变稀疏，导致 50m 以外的物体定位精度迅速下降。

---

## 9. ⚖️ 优缺点总结
- **优点**: 提供了严谨的几何解释，支持任意数量和布置的相机。
- **缺点**: 显存占用大，且对深度估计的精度上限受限于离散 Bin 的划分粒度。

---

## 10. 🛠️ 落地建议
- **算子加速**: 强烈建议使用华为或英伟达优化的 **`VoxelPool` CUDA Kernel** 替代原生 Python 实现，可提速 10 倍以上。
- **显存优化**: 在 Orin 上，建议对深度 Bin 使用 **FP16** 量化，减少张量外积产生的巨大显存开销。
